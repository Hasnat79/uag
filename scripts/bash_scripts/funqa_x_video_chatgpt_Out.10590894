You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:38<00:00, 17.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:38<00:00, 19.07s/it]
/scratch/user/hasnat.md.abdullah/.conda/envs/video_chatgpt/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_projection.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'visual_projection.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm1.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/172 [00:00<?, ?it/s]/scratch/user/hasnat.md.abdullah/.conda/envs/video_chatgpt/lib/python3.10/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  1%|          | 1/172 [01:40<4:45:03, 100.02s/it]  1%|          | 2/172 [01:44<2:03:23, 43.55s/it]   2%|▏         | 3/172 [01:47<1:11:27, 25.37s/it]  2%|▏         | 4/172 [01:52<48:08, 17.19s/it]    3%|▎         | 5/172 [01:54<32:25, 11.65s/it]  3%|▎         | 6/172 [01:58<24:55,  9.01s/it]  4%|▍         | 7/172 [02:56<1:09:19, 25.21s/it]  5%|▍         | 8/172 [02:58<48:26, 17.72s/it]    5%|▌         | 9/172 [03:03<37:01, 13.63s/it]  6%|▌         | 10/172 [03:05<27:25, 10.16s/it]  6%|▋         | 11/172 [03:10<23:03,  8.59s/it]  7%|▋         | 12/172 [03:15<20:04,  7.53s/it]  8%|▊         | 13/172 [03:18<15:53,  6.00s/it]  8%|▊         | 14/172 [03:20<12:48,  4.86s/it]  9%|▊         | 15/172 [03:24<12:06,  4.62s/it]  9%|▉         | 16/172 [03:26<09:42,  3.73s/it] 10%|▉         | 17/172 [03:27<07:59,  3.10s/it] 10%|█         | 18/172 [03:31<08:14,  3.21s/it] 11%|█         | 19/172 [03:35<08:45,  3.43s/it] 12%|█▏        | 20/172 [03:39<09:20,  3.69s/it] 12%|█▏        | 21/172 [03:44<10:08,  4.03s/it] 13%|█▎        | 22/172 [03:48<10:36,  4.24s/it] 13%|█▎        | 23/172 [03:52<09:50,  3.96s/it] 14%|█▍        | 24/172 [03:57<10:55,  4.43s/it] 15%|█▍        | 25/172 [04:01<10:21,  4.23s/it] 15%|█▌        | 26/172 [04:05<09:56,  4.09s/it] 16%|█▌        | 27/172 [04:08<09:27,  3.92s/it] 16%|█▋        | 28/172 [04:10<07:58,  3.32s/it] 17%|█▋        | 29/172 [04:13<07:39,  3.21s/it] 17%|█▋        | 30/172 [04:15<06:33,  2.77s/it] 18%|█▊        | 31/172 [04:18<06:45,  2.87s/it] 19%|█▊        | 32/172 [04:21<07:03,  3.02s/it] 19%|█▉        | 33/172 [04:24<06:46,  2.93s/it] 20%|█▉        | 34/172 [04:28<07:34,  3.30s/it] 20%|██        | 35/172 [04:30<06:41,  2.93s/it] 21%|██        | 36/172 [04:33<06:13,  2.74s/it] 22%|██▏       | 37/172 [04:35<05:55,  2.63s/it] 22%|██▏       | 38/172 [04:38<06:03,  2.71s/it] 23%|██▎       | 39/172 [04:43<07:23,  3.33s/it] 23%|██▎       | 40/172 [04:47<08:10,  3.72s/it] 24%|██▍       | 41/172 [04:50<07:34,  3.47s/it] 24%|██▍       | 42/172 [04:55<08:29,  3.92s/it] 25%|██▌       | 43/172 [05:01<09:48,  4.56s/it] 26%|██▌       | 44/172 [05:06<09:33,  4.48s/it] 26%|██▌       | 45/172 [05:09<08:57,  4.24s/it] 27%|██▋       | 46/172 [05:13<08:45,  4.17s/it] 27%|██▋       | 47/172 [05:16<07:46,  3.73s/it] 28%|██▊       | 48/172 [05:19<07:02,  3.41s/it] 28%|██▊       | 49/172 [05:24<08:21,  4.07s/it] 29%|██▉       | 50/172 [05:26<07:12,  3.55s/it] 30%|██▉       | 51/172 [05:30<06:50,  3.39s/it] 30%|███       | 52/172 [05:34<07:18,  3.66s/it] 31%|███       | 53/172 [05:37<06:47,  3.43s/it] 31%|███▏      | 54/172 [05:40<06:42,  3.41s/it] 32%|███▏      | 55/172 [05:45<07:43,  3.97s/it] 33%|███▎      | 56/172 [05:48<07:12,  3.72s/it] 33%|███▎      | 57/172 [05:51<06:30,  3.40s/it] 34%|███▎      | 58/172 [05:55<06:34,  3.46s/it] 34%|███▍      | 59/172 [05:59<06:52,  3.65s/it] 35%|███▍      | 60/172 [06:02<06:19,  3.38s/it] 35%|███▌      | 61/172 [06:05<06:20,  3.42s/it] 36%|███▌      | 62/172 [06:09<06:27,  3.52s/it] 37%|███▋      | 63/172 [06:12<06:13,  3.43s/it] 37%|███▋      | 64/172 [06:15<05:59,  3.33s/it] 38%|███▊      | 65/172 [06:18<05:38,  3.16s/it] 38%|███▊      | 66/172 [06:20<05:08,  2.91s/it] 39%|███▉      | 67/172 [06:26<06:36,  3.77s/it] 40%|███▉      | 68/172 [06:30<06:38,  3.83s/it] 40%|████      | 69/172 [06:32<05:26,  3.17s/it] 41%|████      | 70/172 [06:35<05:22,  3.16s/it] 41%|████▏     | 71/172 [06:39<05:46,  3.43s/it] 42%|████▏     | 72/172 [06:42<05:38,  3.38s/it] 42%|████▏     | 73/172 [06:45<05:22,  3.26s/it] 43%|████▎     | 74/172 [06:51<06:34,  4.02s/it] 44%|████▎     | 75/172 [06:54<06:16,  3.89s/it] 44%|████▍     | 76/172 [07:00<06:48,  4.25s/it] 45%|████▍     | 77/172 [07:02<06:05,  3.85s/it] 45%|████▌     | 78/172 [07:07<06:08,  3.92s/it] 46%|████▌     | 79/172 [07:09<05:31,  3.57s/it] 47%|████▋     | 80/172 [07:13<05:37,  3.67s/it] 47%|████▋     | 81/172 [07:16<05:12,  3.43s/it] 48%|████▊     | 82/172 [07:20<05:15,  3.51s/it] 48%|████▊     | 83/172 [07:23<05:16,  3.55s/it] 49%|████▉     | 84/172 [07:26<04:34,  3.12s/it] 49%|████▉     | 85/172 [07:29<04:49,  3.33s/it] 50%|█████     | 86/172 [07:32<04:24,  3.07s/it] 51%|█████     | 87/172 [07:37<05:07,  3.61s/it] 51%|█████     | 88/172 [07:40<04:55,  3.52s/it] 52%|█████▏    | 89/172 [07:42<04:09,  3.01s/it] 52%|█████▏    | 90/172 [08:40<26:40, 19.52s/it] 53%|█████▎    | 91/172 [08:44<20:02, 14.84s/it] 53%|█████▎    | 92/172 [08:48<15:32, 11.65s/it] 54%|█████▍    | 93/172 [08:51<11:44,  8.92s/it] 55%|█████▍    | 94/172 [09:49<30:48, 23.70s/it] 55%|█████▌    | 95/172 [09:52<22:27, 17.50s/it] 56%|█████▌    | 96/172 [09:53<16:05, 12.70s/it] 56%|█████▋    | 97/172 [09:56<12:18,  9.85s/it] 57%|█████▋    | 98/172 [10:00<09:47,  7.94s/it] 58%|█████▊    | 99/172 [10:02<07:34,  6.22s/it] 58%|█████▊    | 100/172 [10:06<06:30,  5.43s/it] 59%|█████▊    | 101/172 [10:09<05:34,  4.71s/it] 59%|█████▉    | 102/172 [10:12<05:00,  4.30s/it] 60%|█████▉    | 103/172 [10:19<05:55,  5.15s/it] 60%|██████    | 104/172 [10:22<05:00,  4.42s/it] 61%|██████    | 105/172 [10:28<05:20,  4.79s/it] 62%|██████▏   | 106/172 [10:30<04:37,  4.21s/it] 62%|██████▏   | 107/172 [10:34<04:11,  3.87s/it] 63%|██████▎   | 108/172 [10:37<04:03,  3.81s/it] 63%|██████▎   | 109/172 [10:42<04:17,  4.09s/it] 64%|██████▍   | 110/172 [11:40<21:00, 20.33s/it] 65%|██████▍   | 111/172 [11:43<15:28, 15.22s/it] 65%|██████▌   | 112/172 [11:47<11:45, 11.75s/it] 66%|██████▌   | 113/172 [11:50<09:03,  9.22s/it] 66%|██████▋   | 114/172 [11:56<07:43,  7.99s/it] 67%|██████▋   | 115/172 [12:01<06:52,  7.23s/it] 67%|██████▋   | 116/172 [12:06<06:07,  6.57s/it] 68%|██████▊   | 117/172 [12:10<05:24,  5.90s/it] 69%|██████▊   | 118/172 [12:14<04:42,  5.24s/it] 69%|██████▉   | 119/172 [12:17<04:04,  4.61s/it] 70%|██████▉   | 120/172 [12:21<03:41,  4.27s/it] 70%|███████   | 121/172 [12:24<03:30,  4.13s/it] 71%|███████   | 122/172 [12:29<03:36,  4.32s/it] 72%|███████▏  | 123/172 [12:33<03:25,  4.20s/it] 72%|███████▏  | 124/172 [12:35<02:44,  3.43s/it] 73%|███████▎  | 125/172 [12:39<02:50,  3.64s/it] 73%|███████▎  | 126/172 [12:42<02:42,  3.53s/it] 74%|███████▍  | 127/172 [12:48<03:14,  4.31s/it] 74%|███████▍  | 128/172 [12:51<02:52,  3.91s/it] 75%|███████▌  | 129/172 [12:54<02:27,  3.44s/it] 76%|███████▌  | 130/172 [12:58<02:41,  3.85s/it] 76%|███████▌  | 131/172 [13:04<03:01,  4.43s/it] 77%|███████▋  | 132/172 [13:07<02:42,  4.07s/it] 77%|███████▋  | 133/172 [13:12<02:42,  4.17s/it] 78%|███████▊  | 134/172 [13:17<02:45,  4.35s/it] 78%|███████▊  | 135/172 [13:22<02:47,  4.53s/it] 79%|███████▉  | 136/172 [13:25<02:29,  4.16s/it] 80%|███████▉  | 137/172 [13:31<02:45,  4.74s/it] 80%|████████  | 138/172 [13:33<02:11,  3.86s/it] 81%|████████  | 139/172 [14:31<11:00, 20.02s/it] 81%|████████▏ | 140/172 [14:33<07:49, 14.68s/it] 82%|████████▏ | 141/172 [14:36<05:48, 11.24s/it] 83%|████████▎ | 142/172 [14:41<04:38,  9.27s/it] 83%|████████▎ | 143/172 [14:44<03:34,  7.39s/it] 84%|████████▎ | 144/172 [14:49<03:09,  6.75s/it] 84%|████████▍ | 145/172 [14:53<02:38,  5.86s/it] 85%|████████▍ | 146/172 [14:59<02:37,  6.06s/it] 85%|████████▌ | 147/172 [15:02<02:04,  4.98s/it] 86%|████████▌ | 148/172 [15:03<01:36,  4.01s/it] 87%|████████▋ | 149/172 [15:09<01:44,  4.54s/it] 87%|████████▋ | 150/172 [15:15<01:48,  4.91s/it] 88%|████████▊ | 151/172 [15:20<01:43,  4.92s/it] 88%|████████▊ | 152/172 [15:23<01:27,  4.40s/it] 89%|████████▉ | 153/172 [15:25<01:06,  3.51s/it] 90%|████████▉ | 154/172 [15:27<00:57,  3.20s/it] 90%|█████████ | 155/172 [15:32<01:03,  3.74s/it] 91%|█████████ | 156/172 [15:35<00:57,  3.59s/it] 91%|█████████▏| 157/172 [15:38<00:51,  3.43s/it] 92%|█████████▏| 158/172 [15:40<00:42,  3.02s/it] 92%|█████████▏| 159/172 [15:44<00:41,  3.21s/it] 93%|█████████▎| 160/172 [15:49<00:45,  3.80s/it] 94%|█████████▎| 161/172 [15:53<00:41,  3.75s/it] 94%|█████████▍| 162/172 [15:56<00:36,  3.69s/it] 95%|█████████▍| 163/172 [15:59<00:29,  3.33s/it] 95%|█████████▌| 164/172 [16:02<00:25,  3.19s/it] 96%|█████████▌| 165/172 [16:05<00:21,  3.11s/it] 97%|█████████▋| 166/172 [16:08<00:18,  3.13s/it] 97%|█████████▋| 167/172 [16:13<00:19,  3.82s/it] 98%|█████████▊| 168/172 [16:18<00:16,  4.03s/it] 98%|█████████▊| 169/172 [16:19<00:09,  3.31s/it] 99%|█████████▉| 170/172 [16:24<00:07,  3.70s/it] 99%|█████████▉| 171/172 [16:27<00:03,  3.59s/it]100%|██████████| 172/172 [16:30<00:00,  3.46s/it]100%|██████████| 172/172 [16:30<00:00,  5.76s/it]
