You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 16.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.28s/it]
/scratch/user/hasnat.md.abdullah/.conda/envs/video_chatgpt/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'visual_projection.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'logit_scale', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/172 [00:00<?, ?it/s]/scratch/user/hasnat.md.abdullah/.conda/envs/video_chatgpt/lib/python3.10/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  1%|          | 1/172 [01:02<2:57:31, 62.29s/it]  1%|          | 2/172 [01:05<1:18:15, 27.62s/it]  2%|▏         | 3/172 [01:08<46:06, 16.37s/it]    2%|▏         | 4/172 [01:10<30:21, 10.85s/it]  3%|▎         | 5/172 [01:13<21:27,  7.71s/it]  3%|▎         | 6/172 [01:16<17:28,  6.32s/it]  4%|▍         | 7/172 [01:19<14:03,  5.11s/it]  5%|▍         | 8/172 [01:21<11:29,  4.21s/it]  5%|▌         | 9/172 [01:24<10:33,  3.89s/it]  6%|▌         | 10/172 [01:27<09:04,  3.36s/it]  6%|▋         | 11/172 [01:29<08:33,  3.19s/it]  7%|▋         | 12/172 [01:35<10:12,  3.83s/it]  8%|▊         | 13/172 [01:37<08:50,  3.33s/it]  8%|▊         | 14/172 [02:35<52:17, 19.86s/it]  9%|▊         | 15/172 [02:37<38:03, 14.55s/it]  9%|▉         | 16/172 [02:39<28:16, 10.88s/it] 10%|▉         | 17/172 [02:42<21:28,  8.32s/it] 10%|█         | 18/172 [02:44<16:51,  6.57s/it] 11%|█         | 19/172 [02:49<14:59,  5.88s/it] 12%|█▏        | 20/172 [02:53<13:41,  5.40s/it] 12%|█▏        | 21/172 [02:58<13:22,  5.31s/it] 13%|█▎        | 22/172 [03:01<11:50,  4.74s/it] 13%|█▎        | 23/172 [03:03<09:47,  3.94s/it] 14%|█▍        | 24/172 [03:07<09:40,  3.92s/it] 15%|█▍        | 25/172 [03:10<08:40,  3.54s/it] 15%|█▌        | 26/172 [03:15<09:25,  3.87s/it] 16%|█▌        | 27/172 [03:19<09:34,  3.96s/it] 16%|█▋        | 28/172 [03:21<08:29,  3.54s/it] 17%|█▋        | 29/172 [03:24<07:29,  3.14s/it] 17%|█▋        | 30/172 [03:29<08:57,  3.79s/it] 18%|█▊        | 31/172 [03:32<08:09,  3.47s/it] 19%|█▊        | 32/172 [03:34<07:17,  3.12s/it] 19%|█▉        | 33/172 [03:36<06:22,  2.75s/it] 20%|█▉        | 34/172 [03:38<06:05,  2.65s/it] 20%|██        | 35/172 [03:41<06:04,  2.66s/it] 21%|██        | 36/172 [03:43<05:38,  2.49s/it] 22%|██▏       | 37/172 [03:45<05:22,  2.39s/it] 22%|██▏       | 38/172 [03:48<05:43,  2.56s/it] 23%|██▎       | 39/172 [03:50<05:30,  2.49s/it] 23%|██▎       | 40/172 [03:53<05:26,  2.48s/it] 24%|██▍       | 41/172 [03:55<05:28,  2.51s/it] 24%|██▍       | 42/172 [04:54<41:45, 19.27s/it] 25%|██▌       | 43/172 [05:52<1:06:17, 30.84s/it] 26%|██▌       | 44/172 [05:53<47:10, 22.11s/it]   26%|██▌       | 45/172 [05:57<35:03, 16.56s/it] 27%|██▋       | 46/172 [05:59<25:51, 12.31s/it] 27%|██▋       | 47/172 [06:02<19:31,  9.37s/it] 28%|██▊       | 48/172 [06:04<14:49,  7.17s/it] 28%|██▊       | 49/172 [06:08<12:55,  6.31s/it] 29%|██▉       | 50/172 [06:11<10:24,  5.12s/it] 30%|██▉       | 51/172 [06:14<09:22,  4.65s/it] 30%|███       | 52/172 [06:16<07:49,  3.91s/it] 31%|███       | 53/172 [06:20<07:39,  3.86s/it] 31%|███▏      | 54/172 [06:25<08:14,  4.19s/it] 32%|███▏      | 55/172 [06:28<07:26,  3.82s/it] 33%|███▎      | 56/172 [06:30<06:23,  3.31s/it] 33%|███▎      | 57/172 [06:33<06:21,  3.32s/it] 34%|███▎      | 58/172 [06:36<05:59,  3.15s/it] 34%|███▍      | 59/172 [06:40<06:01,  3.20s/it] 35%|███▍      | 60/172 [06:42<05:20,  2.86s/it] 35%|███▌      | 61/172 [06:44<04:47,  2.59s/it] 36%|███▌      | 62/172 [07:41<35:02, 19.11s/it] 37%|███▋      | 63/172 [07:43<25:31, 14.05s/it] 37%|███▋      | 64/172 [07:46<19:15, 10.70s/it] 38%|███▊      | 65/172 [07:50<15:06,  8.47s/it] 38%|███▊      | 66/172 [07:53<12:16,  6.95s/it] 39%|███▉      | 67/172 [07:57<10:39,  6.09s/it] 40%|███▉      | 68/172 [08:00<08:45,  5.06s/it] 40%|████      | 69/172 [08:03<07:53,  4.60s/it] 41%|████      | 70/172 [08:06<06:53,  4.06s/it] 41%|████▏     | 71/172 [08:08<05:41,  3.38s/it] 42%|████▏     | 72/172 [08:10<05:12,  3.12s/it] 42%|████▏     | 73/172 [08:13<05:06,  3.10s/it] 43%|████▎     | 74/172 [08:18<05:39,  3.46s/it] 44%|████▎     | 75/172 [08:20<04:59,  3.09s/it] 44%|████▍     | 76/172 [08:21<04:11,  2.62s/it] 45%|████▍     | 77/172 [08:25<04:37,  2.93s/it] 45%|████▌     | 78/172 [08:27<04:05,  2.61s/it] 46%|████▌     | 79/172 [08:29<03:46,  2.43s/it] 47%|████▋     | 80/172 [08:31<03:40,  2.40s/it] 47%|████▋     | 81/172 [08:33<03:27,  2.29s/it] 48%|████▊     | 82/172 [08:36<03:22,  2.25s/it] 48%|████▊     | 83/172 [08:39<03:45,  2.53s/it] 49%|████▉     | 84/172 [08:41<03:32,  2.42s/it] 49%|████▉     | 85/172 [08:43<03:16,  2.26s/it] 50%|█████     | 86/172 [08:45<03:07,  2.18s/it] 51%|█████     | 87/172 [08:47<03:01,  2.14s/it] 51%|█████     | 88/172 [08:51<03:52,  2.77s/it] 52%|█████▏    | 89/172 [08:53<03:29,  2.52s/it] 52%|█████▏    | 90/172 [09:50<25:59, 19.02s/it] 53%|█████▎    | 91/172 [09:52<18:46, 13.90s/it] 53%|█████▎    | 92/172 [09:55<13:50, 10.38s/it] 54%|█████▍    | 93/172 [09:57<10:31,  8.00s/it] 55%|█████▍    | 94/172 [10:00<08:23,  6.45s/it] 55%|█████▌    | 95/172 [10:02<06:35,  5.13s/it] 56%|█████▌    | 96/172 [10:07<06:32,  5.17s/it] 56%|█████▋    | 97/172 [10:10<05:30,  4.41s/it] 57%|█████▋    | 98/172 [10:12<04:34,  3.71s/it] 58%|█████▊    | 99/172 [10:16<04:41,  3.86s/it] 58%|█████▊    | 100/172 [10:20<04:31,  3.78s/it] 59%|█████▊    | 101/172 [10:22<04:05,  3.46s/it] 59%|█████▉    | 102/172 [10:27<04:16,  3.67s/it] 60%|█████▉    | 103/172 [10:31<04:29,  3.90s/it] 60%|██████    | 104/172 [10:33<03:52,  3.42s/it] 61%|██████    | 105/172 [10:36<03:41,  3.30s/it] 62%|██████▏   | 106/172 [10:39<03:20,  3.04s/it] 62%|██████▏   | 107/172 [10:42<03:13,  2.97s/it] 63%|██████▎   | 108/172 [10:44<03:01,  2.84s/it] 63%|██████▎   | 109/172 [10:49<03:37,  3.45s/it] 64%|██████▍   | 110/172 [10:51<03:12,  3.10s/it] 65%|██████▍   | 111/172 [10:57<03:59,  3.92s/it] 65%|██████▌   | 112/172 [11:00<03:28,  3.47s/it] 66%|██████▌   | 113/172 [11:04<03:36,  3.68s/it] 66%|██████▋   | 114/172 [11:07<03:27,  3.57s/it] 67%|██████▋   | 115/172 [11:11<03:34,  3.76s/it] 67%|██████▋   | 116/172 [11:19<04:41,  5.03s/it] 68%|██████▊   | 117/172 [11:24<04:24,  4.81s/it] 69%|██████▊   | 118/172 [11:28<04:19,  4.80s/it] 69%|██████▉   | 119/172 [11:31<03:40,  4.16s/it] 70%|██████▉   | 120/172 [11:35<03:27,  3.99s/it] 70%|███████   | 121/172 [11:36<02:50,  3.34s/it] 71%|███████   | 122/172 [11:39<02:31,  3.04s/it] 72%|███████▏  | 123/172 [11:40<02:08,  2.62s/it] 72%|███████▏  | 124/172 [11:43<02:02,  2.55s/it] 73%|███████▎  | 125/172 [11:45<01:57,  2.49s/it] 73%|███████▎  | 126/172 [11:49<02:15,  2.94s/it] 74%|███████▍  | 127/172 [11:54<02:45,  3.68s/it] 74%|███████▍  | 128/172 [11:57<02:23,  3.25s/it] 75%|███████▌  | 129/172 [11:59<02:07,  2.95s/it] 76%|███████▌  | 130/172 [12:02<02:04,  2.96s/it] 76%|███████▌  | 131/172 [12:06<02:19,  3.40s/it] 77%|███████▋  | 132/172 [12:10<02:15,  3.38s/it] 77%|███████▋  | 133/172 [12:12<01:56,  2.99s/it] 78%|███████▊  | 134/172 [12:15<01:50,  2.90s/it] 78%|███████▊  | 135/172 [12:20<02:13,  3.60s/it] 79%|███████▉  | 136/172 [12:24<02:13,  3.70s/it] 80%|███████▉  | 137/172 [12:27<02:03,  3.53s/it] 80%|████████  | 138/172 [12:30<01:58,  3.48s/it] 81%|████████  | 139/172 [12:33<01:45,  3.19s/it] 81%|████████▏ | 140/172 [12:36<01:41,  3.17s/it] 82%|████████▏ | 141/172 [12:39<01:39,  3.20s/it] 83%|████████▎ | 142/172 [12:42<01:33,  3.11s/it] 83%|████████▎ | 143/172 [12:46<01:36,  3.33s/it] 84%|████████▎ | 144/172 [12:53<02:01,  4.34s/it] 84%|████████▍ | 145/172 [12:56<01:47,  3.97s/it] 85%|████████▍ | 146/172 [13:01<01:56,  4.50s/it] 85%|████████▌ | 147/172 [13:05<01:42,  4.11s/it] 86%|████████▌ | 148/172 [13:07<01:26,  3.62s/it] 87%|████████▋ | 149/172 [13:10<01:18,  3.39s/it] 87%|████████▋ | 150/172 [13:14<01:16,  3.46s/it] 88%|████████▊ | 151/172 [13:16<01:03,  3.04s/it] 88%|████████▊ | 152/172 [13:18<00:56,  2.84s/it] 89%|████████▉ | 153/172 [13:20<00:50,  2.64s/it] 90%|████████▉ | 154/172 [13:22<00:44,  2.45s/it] 90%|█████████ | 155/172 [13:28<00:57,  3.39s/it] 91%|█████████ | 156/172 [13:31<00:53,  3.33s/it] 91%|█████████▏| 157/172 [13:33<00:46,  3.10s/it] 92%|█████████▏| 158/172 [13:36<00:41,  2.98s/it] 92%|█████████▏| 159/172 [13:40<00:41,  3.17s/it] 93%|█████████▎| 160/172 [13:43<00:38,  3.21s/it] 94%|█████████▎| 161/172 [13:47<00:37,  3.40s/it] 94%|█████████▍| 162/172 [13:49<00:29,  3.00s/it] 95%|█████████▍| 163/172 [13:52<00:26,  2.94s/it] 95%|█████████▌| 164/172 [13:54<00:20,  2.58s/it] 96%|█████████▌| 165/172 [13:56<00:17,  2.57s/it] 97%|█████████▋| 166/172 [14:00<00:16,  2.83s/it] 97%|█████████▋| 167/172 [14:03<00:14,  2.99s/it] 98%|█████████▊| 168/172 [14:06<00:12,  3.09s/it] 98%|█████████▊| 169/172 [14:12<00:11,  3.79s/it] 99%|█████████▉| 170/172 [14:17<00:08,  4.25s/it] 99%|█████████▉| 171/172 [14:20<00:03,  3.90s/it]100%|██████████| 172/172 [14:25<00:00,  4.21s/it]100%|██████████| 172/172 [14:25<00:00,  5.03s/it]
