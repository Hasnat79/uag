Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.96s/it]
  0%|          | 0/104 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
 90%|█████████ | 94/104 [00:29<00:03,  3.21it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
 91%|█████████▏| 95/104 [00:41<00:04,  2.03it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
 92%|█████████▏| 96/104 [00:51<00:05,  1.46it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
 93%|█████████▎| 97/104 [01:01<00:06,  1.03it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
 94%|█████████▍| 98/104 [01:10<00:07,  1.26s/it]100%|██████████| 104/104 [01:10<00:00,  1.48it/s]
