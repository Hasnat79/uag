You are using a model of type llava to instantiate a model of type VideoChatGPT. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]
/scratch/user/hasnat.md.abdullah/.conda/envs/video_chatgpt/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'logit_scale', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias']
- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/172 [00:00<?, ?it/s]/scratch/user/hasnat.md.abdullah/.conda/envs/video_chatgpt/lib/python3.10/site-packages/transformers/generation/utils.py:1211: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
  1%|          | 1/172 [00:21<1:02:04, 21.78s/it]  1%|          | 2/172 [00:24<30:36, 10.80s/it]    2%|▏         | 3/172 [00:28<21:16,  7.55s/it]  2%|▏         | 4/172 [00:30<15:09,  5.41s/it]  3%|▎         | 5/172 [00:33<12:03,  4.33s/it]  3%|▎         | 6/172 [00:34<09:37,  3.48s/it]  4%|▍         | 7/172 [00:38<09:14,  3.36s/it]  5%|▍         | 8/172 [00:40<08:10,  2.99s/it]  5%|▌         | 9/172 [00:43<08:25,  3.10s/it]  6%|▌         | 10/172 [00:46<08:34,  3.17s/it]  6%|▋         | 11/172 [00:49<07:45,  2.89s/it]  7%|▋         | 12/172 [00:53<08:41,  3.26s/it]  8%|▊         | 13/172 [00:58<09:54,  3.74s/it]  8%|▊         | 14/172 [01:01<09:14,  3.51s/it]  9%|▊         | 15/172 [01:02<07:52,  3.01s/it]  9%|▉         | 16/172 [01:05<07:04,  2.72s/it] 10%|▉         | 17/172 [01:06<06:26,  2.49s/it] 10%|█         | 18/172 [01:09<06:11,  2.42s/it] 11%|█         | 19/172 [01:11<06:08,  2.41s/it] 12%|█▏        | 20/172 [01:14<06:32,  2.58s/it] 12%|█▏        | 21/172 [01:16<06:16,  2.50s/it] 13%|█▎        | 22/172 [01:21<08:01,  3.21s/it] 13%|█▎        | 23/172 [01:25<08:37,  3.47s/it] 14%|█▍        | 24/172 [01:29<09:03,  3.67s/it] 15%|█▍        | 25/172 [01:32<08:06,  3.31s/it] 15%|█▌        | 26/172 [01:34<07:27,  3.07s/it] 16%|█▌        | 27/172 [01:37<06:40,  2.76s/it] 16%|█▋        | 28/172 [01:39<06:29,  2.71s/it] 17%|█▋        | 29/172 [01:42<06:26,  2.71s/it] 17%|█▋        | 30/172 [01:46<07:08,  3.01s/it] 18%|█▊        | 31/172 [01:50<08:25,  3.58s/it] 19%|█▊        | 32/172 [01:53<07:23,  3.17s/it] 19%|█▉        | 33/172 [01:54<06:18,  2.72s/it] 20%|█▉        | 34/172 [01:56<05:46,  2.51s/it] 20%|██        | 35/172 [01:59<05:45,  2.52s/it] 21%|██        | 36/172 [02:01<05:28,  2.41s/it] 22%|██▏       | 37/172 [02:03<05:14,  2.33s/it] 22%|██▏       | 38/172 [02:06<05:14,  2.35s/it] 23%|██▎       | 39/172 [02:09<05:54,  2.67s/it] 23%|██▎       | 40/172 [02:11<05:21,  2.44s/it] 24%|██▍       | 41/172 [02:13<05:15,  2.40s/it] 24%|██▍       | 42/172 [02:16<05:27,  2.52s/it] 25%|██▌       | 43/172 [02:20<06:17,  2.92s/it] 26%|██▌       | 44/172 [02:22<05:40,  2.66s/it] 26%|██▌       | 45/172 [02:25<06:06,  2.88s/it] 27%|██▋       | 46/172 [02:27<05:29,  2.62s/it] 27%|██▋       | 47/172 [02:29<05:09,  2.48s/it] 28%|██▊       | 48/172 [02:32<05:15,  2.55s/it] 28%|██▊       | 49/172 [02:34<04:55,  2.40s/it] 29%|██▉       | 50/172 [02:39<06:20,  3.12s/it] 30%|██▉       | 51/172 [02:42<06:06,  3.03s/it] 30%|███       | 52/172 [02:44<05:28,  2.73s/it] 31%|███       | 53/172 [02:46<05:02,  2.55s/it] 31%|███▏      | 54/172 [02:48<04:40,  2.38s/it] 32%|███▏      | 55/172 [02:52<05:20,  2.74s/it] 33%|███▎      | 56/172 [02:54<04:52,  2.52s/it] 33%|███▎      | 57/172 [02:56<05:04,  2.64s/it] 34%|███▎      | 58/172 [03:01<06:00,  3.16s/it] 34%|███▍      | 59/172 [03:04<05:55,  3.14s/it] 35%|███▍      | 60/172 [03:07<05:59,  3.21s/it] 35%|███▌      | 61/172 [03:09<05:11,  2.81s/it] 36%|███▌      | 62/172 [03:12<05:00,  2.73s/it] 37%|███▋      | 63/172 [03:14<04:38,  2.56s/it] 37%|███▋      | 64/172 [03:17<04:45,  2.64s/it] 38%|███▊      | 65/172 [03:19<04:26,  2.49s/it] 38%|███▊      | 66/172 [03:21<04:16,  2.42s/it] 39%|███▉      | 67/172 [03:23<04:07,  2.36s/it] 40%|███▉      | 68/172 [03:25<03:51,  2.23s/it] 40%|████      | 69/172 [03:32<05:57,  3.47s/it] 41%|████      | 70/172 [03:34<05:06,  3.01s/it] 41%|████▏     | 71/172 [03:37<05:30,  3.27s/it] 42%|████▏     | 72/172 [03:40<04:51,  2.91s/it] 42%|████▏     | 73/172 [03:42<04:46,  2.89s/it] 43%|████▎     | 74/172 [03:46<04:54,  3.01s/it] 44%|████▎     | 75/172 [03:48<04:19,  2.68s/it] 44%|████▍     | 76/172 [03:49<03:43,  2.32s/it] 45%|████▍     | 77/172 [03:54<04:48,  3.04s/it] 45%|████▌     | 78/172 [03:56<04:12,  2.69s/it] 46%|████▌     | 79/172 [03:58<04:14,  2.73s/it] 47%|████▋     | 80/172 [04:00<03:49,  2.49s/it] 47%|████▋     | 81/172 [04:02<03:28,  2.29s/it] 48%|████▊     | 82/172 [04:04<03:23,  2.27s/it] 48%|████▊     | 83/172 [04:07<03:19,  2.24s/it] 49%|████▉     | 84/172 [04:09<03:17,  2.24s/it] 49%|████▉     | 85/172 [04:12<03:34,  2.47s/it] 50%|█████     | 86/172 [04:14<03:17,  2.29s/it] 51%|█████     | 87/172 [04:16<03:20,  2.36s/it] 51%|█████     | 88/172 [04:20<03:46,  2.69s/it] 52%|█████▏    | 89/172 [04:22<03:32,  2.56s/it] 52%|█████▏    | 90/172 [04:24<03:24,  2.49s/it] 53%|█████▎    | 91/172 [04:27<03:36,  2.67s/it] 53%|█████▎    | 92/172 [04:29<03:17,  2.47s/it] 54%|█████▍    | 93/172 [04:32<03:16,  2.49s/it] 55%|█████▍    | 94/172 [04:34<03:02,  2.34s/it] 55%|█████▌    | 95/172 [04:36<02:54,  2.26s/it] 56%|█████▌    | 96/172 [04:38<02:49,  2.23s/it] 56%|█████▋    | 97/172 [04:41<03:12,  2.57s/it] 57%|█████▋    | 98/172 [04:43<02:56,  2.38s/it] 58%|█████▊    | 99/172 [04:46<02:56,  2.41s/it] 58%|█████▊    | 100/172 [04:48<02:54,  2.42s/it] 59%|█████▊    | 101/172 [04:51<03:04,  2.60s/it] 59%|█████▉    | 102/172 [04:55<03:15,  2.79s/it] 60%|█████▉    | 103/172 [04:58<03:32,  3.07s/it] 60%|██████    | 104/172 [05:02<03:38,  3.21s/it] 61%|██████    | 105/172 [05:05<03:33,  3.18s/it] 62%|██████▏   | 106/172 [05:07<03:07,  2.85s/it] 62%|██████▏   | 107/172 [05:12<03:40,  3.39s/it] 63%|██████▎   | 108/172 [05:17<04:06,  3.86s/it] 63%|██████▎   | 109/172 [05:20<03:51,  3.67s/it] 64%|██████▍   | 110/172 [05:22<03:25,  3.31s/it] 65%|██████▍   | 111/172 [05:26<03:25,  3.37s/it] 65%|██████▌   | 112/172 [05:28<03:07,  3.13s/it] 66%|██████▌   | 113/172 [05:31<02:59,  3.04s/it] 66%|██████▋   | 114/172 [05:34<02:46,  2.87s/it] 67%|██████▋   | 115/172 [05:37<02:45,  2.90s/it] 67%|██████▋   | 116/172 [05:41<02:58,  3.19s/it] 68%|██████▊   | 117/172 [05:44<02:58,  3.25s/it] 69%|██████▊   | 118/172 [05:47<02:55,  3.25s/it] 69%|██████▉   | 119/172 [05:51<02:54,  3.29s/it] 70%|██████▉   | 120/172 [05:54<02:50,  3.27s/it] 70%|███████   | 121/172 [05:56<02:36,  3.08s/it] 71%|███████   | 122/172 [06:00<02:39,  3.20s/it] 72%|███████▏  | 123/172 [06:02<02:23,  2.92s/it] 72%|███████▏  | 124/172 [06:04<02:10,  2.71s/it] 73%|███████▎  | 125/172 [06:08<02:24,  3.08s/it] 73%|███████▎  | 126/172 [06:12<02:25,  3.17s/it] 74%|███████▍  | 127/172 [06:17<02:48,  3.75s/it] 74%|███████▍  | 128/172 [07:15<14:43, 20.08s/it] 75%|███████▌  | 129/172 [07:17<10:33, 14.74s/it] 76%|███████▌  | 130/172 [07:20<07:48, 11.14s/it] 76%|███████▌  | 131/172 [07:22<05:47,  8.46s/it] 77%|███████▋  | 132/172 [07:25<04:31,  6.80s/it] 77%|███████▋  | 133/172 [07:28<03:32,  5.45s/it] 78%|███████▊  | 134/172 [07:31<03:01,  4.78s/it] 78%|███████▊  | 135/172 [07:36<03:00,  4.89s/it] 79%|███████▉  | 136/172 [07:39<02:34,  4.30s/it] 80%|███████▉  | 137/172 [07:42<02:17,  3.94s/it] 80%|████████  | 138/172 [07:45<02:10,  3.83s/it] 81%|████████  | 139/172 [07:48<01:51,  3.39s/it] 81%|████████▏ | 140/172 [07:51<01:41,  3.19s/it] 82%|████████▏ | 141/172 [07:53<01:33,  3.00s/it] 83%|████████▎ | 142/172 [07:55<01:23,  2.80s/it] 83%|████████▎ | 143/172 [07:59<01:28,  3.04s/it] 84%|████████▎ | 144/172 [08:04<01:42,  3.66s/it] 84%|████████▍ | 145/172 [08:07<01:34,  3.49s/it] 85%|████████▍ | 146/172 [08:13<01:47,  4.14s/it] 85%|████████▌ | 147/172 [08:18<01:51,  4.45s/it] 86%|████████▌ | 148/172 [08:21<01:32,  3.85s/it] 87%|████████▋ | 149/172 [08:23<01:17,  3.37s/it] 87%|████████▋ | 150/172 [08:26<01:09,  3.18s/it] 88%|████████▊ | 151/172 [08:29<01:05,  3.14s/it] 88%|████████▊ | 152/172 [08:32<01:02,  3.12s/it] 89%|████████▉ | 153/172 [08:34<00:57,  3.01s/it] 90%|████████▉ | 154/172 [08:36<00:47,  2.66s/it] 90%|█████████ | 155/172 [08:40<00:49,  2.92s/it] 91%|█████████ | 156/172 [08:43<00:49,  3.07s/it] 91%|█████████▏| 157/172 [08:46<00:43,  2.87s/it] 92%|█████████▏| 158/172 [08:48<00:39,  2.80s/it] 92%|█████████▏| 159/172 [08:51<00:34,  2.67s/it] 93%|█████████▎| 160/172 [08:54<00:35,  2.98s/it] 94%|█████████▎| 161/172 [08:59<00:38,  3.48s/it] 94%|█████████▍| 162/172 [09:02<00:32,  3.24s/it] 95%|█████████▍| 163/172 [09:04<00:25,  2.84s/it] 95%|█████████▌| 164/172 [09:05<00:20,  2.51s/it] 96%|█████████▌| 165/172 [09:07<00:16,  2.36s/it] 97%|█████████▋| 166/172 [09:10<00:14,  2.42s/it] 97%|█████████▋| 167/172 [09:12<00:11,  2.25s/it] 98%|█████████▊| 168/172 [09:14<00:08,  2.11s/it] 98%|█████████▊| 169/172 [09:16<00:06,  2.33s/it] 99%|█████████▉| 170/172 [09:18<00:04,  2.11s/it] 99%|█████████▉| 171/172 [09:21<00:02,  2.47s/it]100%|██████████| 172/172 [09:23<00:00,  2.33s/it]100%|██████████| 172/172 [09:23<00:00,  3.28s/it]
